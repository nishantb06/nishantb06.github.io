{
      "id": "bigquery-rows-go",
      "title": "Pushing New Rows to BigQuery Using Go",
      "subtitle": "A Guide to Inserting Data into BigQuery Tables with Go",
      "slug": "bigquery-rows-go",
      "author": "Nishant Bhansali",
      "date": "2024-03-15",
      "tags": ["Go", "BigQuery", "Google Cloud Platform", "Data Insertion"],
      "isShortArticle": false,
      "schema": [
        {
          "Introduction": []
        },
        {
          "Prerequisites": []
        },
        {
          "Template": []
        },
        {
          "Understanding the Code": [
            "Struct Definition",
            "Client Creation",
            "Table Specification",
            "Metadata Retrieval",
            "Inserter Creation",
            "Data Preparation",
            "Data Insertion"
          ]
        },
        {
          "Conclusion": []
        }
      ],
      "content": [
        {
          "type": "h2",
          "className": "title",
          "content": "Introduction"
        },
        {
          "type": "p",
          "content": "In this blog post, we'll explore how to push new rows into a BigQuery table using Go. BigQuery, a serverless and highly-scalable data warehouse, is a part of Google Cloud Platform (GCP). We will be using the `cloud.google.com/go/bigquery` package for Go to interact with BigQuery."
        },
        {
          "type": "h2",
          "className": "title",
          "content": "Prerequisites"
        },
        {
          "type": "p",
          "content": "Before diving into the code, make sure you have the following set up:"
        },
        {
          "type": "ul",
          "content": [
            {
              "type": "li",
              "content": "A GCP project with BigQuery enabled"
            },
            {
              "type": "li",
              "content": "Service account credentials in a JSON file"
            },
            {
              "type": "li",
              "content": "Go installed on your machine"
            }
          ]
        },
        {
          "type": "h2",
          "className": "title",
          "content": "Template"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"cloud.google.com/go/bigquery\"\n\t\"google.golang.org/api/option\"\n)\n\n// SampleData represents the structure of the data to be inserted into BigQuery.\ntype SampleData struct {\n\tName  string  `bigquery:\"name\"`   // Name of the data\n\tValue float32 `bigquery:\"value\"`  // Value of the data\n\tTime  string  `bigquery:\"time\"`   // Time when the data was recorded\n}\n\nvar credentialsFile = \"path-to-credentials.json\" // Path to the JSON credentials file\n\n// BatchPushToBigQuery is a function that inserts data into BigQuery in batches.\nfunc BatchPushToBigQuery() {\n\tctx := context.Background()\n\n\t// Create a new BigQuery client using the provided credentials file\n\tclient, err := bigquery.NewClient(ctx, \"your-project-id\", option.WithCredentialsFile(credentialsFile))\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create BigQuery client: %v\", err)\n\t}\n\n\tdataset := client.Dataset(\"dataset-name\") // Specify the dataset name\n\ttable := dataset.Table(\"table-name\")     // Specify the table name\n\n\t// Retrieve the schema for the specified table\n\tmeta, err := table.Metadata(ctx)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to get table meta data: %v\", err)\n\t}\n\n\t// Print the schema of the table\n\tfor _, field := range meta.Schema {\n\t\tfmt.Printf(\"Field Name: %s, Field Type: %s\\n\", field.Name, field.Type)\n\t}\n\n\t// Create sample data to be inserted into the table\n\tdata := SampleData{\n\t\tName:  \"nishant\",\n\t\tValue: 123.123,\n\t\tTime:  time.Now().Format(time.RFC3339),\n\t}\n\n\tdata2 := SampleData{\n\t\tName:  \"nishant2\",\n\t\tValue: 123.123,\n\t\tTime:  time.Now().Format(time.RFC3339),\n\t}\n\n\tinserter := table.Inserter() // Create an inserter for the table\n\titems := []*SampleData{\n\t\t&data,\n\t\t&data2,\n\t}\n\n\t// Insert the data into the table\n\tif err := inserter.Put(ctx, items); err != nil {\n\t\tlog.Fatalf(\"Failed to insert data: %v\", err)\n\t}\n\n\tfmt.Println(\"Data inserted successfully!\")\n}"
            }
          ]
        },
        {
          "type": "h2",
          "className": "title",
          "content": "Understanding the Code"
        },
        {
          "type": "p",
          "content": "Let's break down the code you've provided:"
        },
        {
          "type": "h3",
          "className": "title",
          "content": "Struct Definition"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "type SampleData struct {\n    Name  string  `bigquery:\"name\"`   // Name of the data\n    Value float32 `bigquery:\"value\"`  // Value of the data\n    Time  string  `bigquery:\"time\"`   // Time when the data was recorded\n}"
            }
          ]
        },
        {
          "type": "p",
          "content": "The `SampleData` struct represents the structure of the data to be inserted into BigQuery. The struct tags (`bigquery:\"name\"`, `bigquery:\"value\"`, and `bigquery:\"time\"`) provide metadata to the BigQuery API, mapping the fields of the struct to the corresponding columns in the BigQuery table."
        },
        {
          "type": "h3",
          "className": "title",
          "content": "Client Creation"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "client, err := bigquery.NewClient(ctx, \"your-project-id\", option.WithCredentialsFile(credentialsFile))"
            }
          ]
        },
        {
          "type": "p",
          "content": "The `bigquery.NewClient` function is used to create a new BigQuery client. It takes the context (`ctx`), project ID, and options (in this case, the path to the JSON credentials file)."
        },
        {
          "type": "h3",
          "className": "title",
          "content": "Table Specification"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "dataset := client.Dataset(\"dataset-name\")\ntable := dataset.Table(\"table-name\")"
            }
          ]
        },
        {
          "type": "p",
          "content": "Here, we specify the dataset and table within BigQuery where we want to insert the data."
        },
        {
          "type": "h3",
          "className": "title",
          "content": "Metadata Retrieval"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "meta, err := table.Metadata(ctx)"
            }
          ]
        },
        {
          "type": "p",
          "content": "We retrieve the schema (metadata) of the specified table. The schema includes information about the columns and their types."
        },
        {
          "type": "h3",
          "className": "title",
          "content": "Inserter Creation"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "inserter := table.Inserter()"
            }
          ]
        },
        {
          "type": "p",
          "content": "An inserter is created for the table, which will be used to insert data."
        },
        {
          "type": "h3",
          "className": "title",
          "content": "Data Preparation"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "data := SampleData{\n    Name:  \"nishant\",\n    Value: 123.123,\n    Time:  time.Now().Format(time.RFC3339),\n}"
            }
          ]
        },
        {
          "type": "p",
          "content": "Sample data is created using the `SampleData` struct. Note that even if the column datatype is `timestamp` in BigQuery, we have to push a string with proper formatting from our side, which is why we are not using a `time.Time` datatype for this field and using `string` instead."
        },
        {
          "type": "h3",
          "className": "title",
          "content": "Data Insertion"
        },
        {
          "type": "pre",
          "content": [
            {
              "type": "code",
              "content": "items := []*SampleData{\n    &data,\n    &data2,\n}\n\nif err := inserter.Put(ctx, items); err != nil {\n    log.Fatalf(\"Failed to insert data: %v\", err)\n}"
            }
          ]
        },
        {
          "type": "p",
          "content": "An array of data items is created to be inserted into the table. The data is then inserted into the specified table using the `Put` method of the inserter."
        },
        {
          "type": "h2",
          "className": "title",
          "content": "Conclusion"
        },
        {
          "type": "p",
          "content": "In this blog post, we've covered the basics of pushing new rows into a BigQuery table using Go. Understanding the structure of your data, creating a BigQuery client, and utilising the provided libraries for interaction are crucial steps in achieving successful data insertion. This example can be extended for more complex scenarios, such as streaming data or updating existing records. Make sure to check the official documentation for more details and advanced usage."
        },
        {
          "type": "p",
          "content": "Note that stopping the instance won't delete the EBS volume, and you will continue to be billed for it. However, terminating the instance should delete the associated EBS volume as well."
        },
        {
          "type": "p",
          "content": "When you stop and start an EC2 instance, its public IP address changes. This means you won't be able to SSH into the instance using the same command as before. To overcome this, you can hibernate your instance, but be aware that this feature is not available for all instance types."
        },
        {
          "type": "p",
          "content": "For a more secure and convenient way to access your EC2 instance, you can add your own public key to the instance's authorized keys. Here's how:"
        },
        {
          "type": "p",
          "content": "1. SSH into your EC2 instance using the .pem file."
        },
        {
          "type": "p",
          "content": "2. Navigate to the ~/.ssh directory: cd ~/.ssh"
        },
        {
          "type": "p",
          "content": "3. Open the authorized_keys file: nano authorized_keys"
        },
        {
          "type": "p",
          "content": "4. Paste your public key at the end of the file, then save and exit."
        },
        {
          "type": "p",
          "content": "After doing this, you can SSH into your instance using your own key pair, without needing the .pem file provided by AWS."
        },
        {
          "type": "p",
          "content": "Important: Be cautious with the .pem file provided by AWS. If shared, it could allow others to access your EC2 instance. By using your own public key as described above, you enhance the security of your instance."
        }
  ]
}