{
  "id": "the-annotated-llama",
  "title": "The Annotated Llama",
  "subtitle": "An in-depth look at the Llama model architecture, including its components and design choices",
  "slug": "the-annotated-llama",
  "author": "Nishant Bhansali",
  "date": "2023-04-15",
  "tags": ["LLM's"],
  "isShortArticle": false,
  "schema": [
    {
      "Foreword": []
    },
    {
      "Introduction": [
        "LLM Scaling Laws",
        "Datasets used",
        "Architecture"
      ]
    },
    {
        "How to download the Weights on your Machine": []
    },
    {
      "Code deep dive": [
        {"Tokenizer": ["Byte pair encoding algorithm"]},
        {"Model": ["Transformer()", "TransformerBlock()", "FeedForward()", "Attention()"]},
        {"Generation": ["Sampling from top p probabilites", "Post Processing function"]}
      ]
    },
    {
      "Conclusion": []
    }
  ],
  "content": [
    {
      "type": "h2",
      "className": "title",
      "content": "Foreword"
    },
    {
      "type": "p",
      "content": "Welcome to **“The Annotated LLaMA”**"
    },
    {
      "type": "p",
      "content": "One of the most brilliant and well-explained articles I have read is [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) and the [Annotated DETR](https://amaarora.github.io/posts/2021-07-26-annotateddetr.html). It introduced **Attention** like no other post. The simple idea was to present an “annotated” version of the paper [Attention is all you need](https://arxiv.org/abs/1706.03762) along with code."
    },
    {
      "type": "p",
      "content": "I have tried to do something similar with the LLaMA models by Meta Research, without which the commercial use of many Large Language models would not have been possible."
    },
    {
      "type": "p",
      "content": "[arXiv](https://arxiv.org/abs/2302.13971v1) , [Official Code](https://github.com/facebookresearch/llama)"
    },
    {
      "type": "img",
      "src": "https://images.unsplash.com/photo-1589182337358-2cb63099350c?ixlib=rb-4.0.3&q=85&fm=jpg&crop=entropy&cs=srgb&w=4800",
      "alt": "Llama"
    }
  ]
}