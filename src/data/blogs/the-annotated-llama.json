{
  "id": "the-annotated-llama",
  "title": "The Annotated Llama",
  "subtitle": "An in-depth look at the Llama model architecture, including its components and design choices",
  "slug": "the-annotated-llama",
  "author": "Nishant Bhansali",
  "date": "2023-04-15",
  "tags": ["LLM's"],
  "isShortArticle": false,
  "schema": [
    {
      "Foreword": []
    },
    {
      "Introduction": [
        "LLM Scaling Laws",
        "Datasets used",
        "Architecture"
      ]
    },
    {
        "How to download the Weights on your Machine": []
    },
    {
      "Code deep dive": [
        {"Tokenizer": ["Byte pair encoding algorithm"]},
        {"Model": ["Transformer()", "TransformerBlock()", "FeedForward()", "Attention()"]},
        {"Generation": ["Sampling from top p probabilites", "Post Processing function"]}
      ]
    },
    {
      "Conclusion": []
    }
  ],
  "content": [
    {
      "type": "h2",
      "className": "title",
      "content": "Foreword"
    },
    {
      "type": "p",
      "content": "Welcome to **â€œThe Annotated LLaMAâ€**"
    },
    {
      "type": "p",
      "content": "One of the most brilliant and well-explained articles I have read isÂ [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) and the [Annotated DETR](https://amaarora.github.io/posts/2021-07-26-annotateddetr.html). It introducedÂ **Attention**Â like no other post. The simple idea was to present an â€œannotatedâ€ version of the paperÂ [Attention is all you need](https://arxiv.org/abs/1706.03762)Â along with code."
    },
    {
      "type": "p",
      "content": "I have tried to do something similar with the LLaMA models by Meta Research, without which the commercial use of many Large Language models would not have been possible."
    },
    {
      "type": "p",
      "content": "[arXiv](https://arxiv.org/abs/2302.13971v1) , [Official Code](https://github.com/facebookresearch/llama)"
    },
    {
      "type": "img",
      "src": "/blogs/images/llama.jpeg",
      "alt": "Local Llama image"
    },
    {
      "type": "h2",
      "className": "title",
      "content": "Introduction"
    },
    {
      "type": "p",
      "content": "Before Llama came there were a series of really good Large Language Models (LLM's) like Chinchilla, PaLM and GPT-3, only problem they were trained on proprietary data and were not accessible to the public for research or commercial use."
    },
    {
      "type": "p",
      "content": "On February 27, 2023, Facebook released a set of models called LLaMA models that had:"
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "Performance comparable to State-of-the-art LLM models at that time"
        },
        {
          "type": "li",
          "content": "Trained entirely on Publicly available data"
        },
        {
          "type": "li",
          "content": "Sizes as small as 7B parameter to as Large as 65B parameters"
        },
        {
          "type": "li",
          "content": "Models were available publicly for research purposes only (Not for Commercial Use). Though these models were leaked later"
        },
        {
          "type": "li",
          "content": "Inference Code was Open sourced, (not the training code ðŸ˜“)"
        }
      ]
    },
    {
      "type": "p",
      "content": "With these models, they prove that:"
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "It is possible to train SOTA LLM's without the use proprietary and inaccessible datasets"
        },
        {
          "type": "li",
          "content": "The performance of a model as small as 7B parameters will keep on increasing as with the size of the dataset it is trained on"
        },
        {
          "type": "li",
          "content": "They wanted to train and optimise a set of models for best possible performance at fixed Inference budgets, by training on more tokens than what is typically used"
        }
      ]
    },
    {
      "type": "blockquote",
      "content": "For instance, although [Hoffmann et al. (2022)](https://arxiv.org/abs/2203.15556) recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "LLM Scaling Laws"
    },
    {
      "type": "p",
      "content": "Before we dive into training, it's important to cover how LLMs scale. Understanding scaling lets us effectively balance the size and complexity of your model and the size of the data you'll use to train it."
    },
    {
      "type": "p",
      "content": "Some relevant history here: OpenAI originally introduced \"the LLM scaling laws\" in 2020. They suggested that increasing model size was more important than scaling data size. This held for about two years before DeepMind suggested almost the polar opposite: that previous models were significantly undertrained and that increasing your foundational training datasets actually leads to better performance."
    },
    {
      "type": "p",
      "content": "That changed in 2022. Specifically, DeepMind put forward an alternative approach in their Training Compute-Optimal Large Language Models paper. They found that current LLMs are actually significantly undertrained. Put simply: these large models weren't trained on nearly enough data."
    },
    {
      "type": "p",
      "content": "Deepmind showcased this with a model called Chinchilla, which is a fourth the size of the Gopher model above but trained on 4.6x more data. At that reduced size but with far more training data, Chinchilla outperformed Gopher and other LLMs."
    },
    {
      "type": "p",
      "content": "DeepMind claims that the model size and the number of training tokens should instead increase at roughly the same rate to achieve optimal performance. If you get a 10x increase in compute, you should make your model 3.1x times bigger and the data you train over 3.1x bigger; if you get a 100x increase in compute, you should make your model 10x bigger and your data 10x bigger."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Datasets Used"
    },
    {
      "type": "p",
      "content": "The LLaMA models were pretrained on a massive dataset of approximately 4.3 TB. Here's a breakdown of the datasets used:"
    },
    {
      "type": "img",
      "src": "https://i.imgur.com/DhWpc17.png",
      "alt": "Datasets used for LLaMA pretraining"
    },
    {
      "type": "p",
      "content": "Key points about the datasets:"
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "Wikipedia and Books datasets were used for approximately 2 epochs"
        },
        {
          "type": "li",
          "content": "Other datasets were used for only 1 epoch"
        },
        {
          "type": "li",
          "content": "The total size of all datasets combined is about 4.3 TB"
        }
      ]
    },
    {
      "type": "p",
      "content": "In summary, they trained a large transformer model on this extensive dataset using a standard optimizer (AdamW). This approach of using a large quantity of diverse, high-quality data for pretraining is crucial for achieving state-of-the-art performance in large language models."
    },
    {
      "type": "h2",
      "className": "title",
      "content": "Architecture"
    },
    {
      "type": "p",
      "content": "The LLaMA model introduced several major changes to the standard Transformer architecture:"
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "Using RMSNorm instead of LayerNorm"
        },
        {
          "type": "li",
          "content": "Using Rotary Positional Embeddings (which are relative and not absolute)"
        },
        {
          "type": "li",
          "content": "Caching of keys and values during the attention mechanism"
        },
        {
          "type": "li",
          "content": "SwiGLU activation function"
        }
      ]
    },
    {
      "type": "h2",
      "className": "title",
      "content": "How to Download the Weights on Your Machine"
    },
    {
      "type": "p",
      "content": "The easiest way to download the weights on your machine is using the pyllama package. Here are the steps involved:"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "Fork the repository and git clone it to your system."
        },
        {
          "type": "li",
          "content": "Install the pyllama package with pip install pyllama -U"
        },
        {
          "type": "li",
          "content": "To download the 7B model, use python -m llama.download --model_size 7B"
        }
      ]
    },
    {
      "type": "p",
      "content": "Note: Some users have reported issues where the download stops after a few minutes and needs to be restarted manually. If you encounter this problem, you can use a shell script to automate the process. Credit for this solution goes to a comment on this GitHub issue. Credits to this [comment](https://github.com/juncongmoo/pyllama/issues/104#issuecomment-1588856820) on this [issue](https://github.com/juncongmoo/pyllama/issues/104). "
    },
    {
      "type": "p",
      "content": "Here's a shell script that can help automate the download process and handle interruptions:"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "#!/bin/bash\n# Function to handle stopping the script\nfunction stop_script() {\n  echo \"Stopping the script.\"\n  exit 0\n}\n\n# Register the signal handler\ntrap stop_script SIGINT\n\nwhile true; do\n  # Run the command with a timeout of 200 seconds\n  timeout 200  python -m llama.download --model_size $1 --folder model\n\n  echo \"restart download\"\n  sleep 1  # Wait for 1 second before starting the next iteration\n# Wait for any key to be pressed within a 1-second timeout\n  read -t 1 -n 1 -s key\n  if [[ $key ]]; then\n    stop_script\n  fi\ndone"
        }
      ]
    },
    {
      "type": "p",
      "content": "Save this script as 'llama_download.sh' and make it executable with 'chmod +x llama_download.sh'. You can then run it using:"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "bash llama_download.sh 7B"
        }
      ]
    },
    {
      "type": "p",
      "content": "This script will continuously attempt to download the model, restarting if it encounters an interruption. You can stop the script at any time by pressing any key."
    },
    {
      "type": "p",
      "content": "After Successful Download"
    },
    {
      "type": "p",
      "content": "After a successful download, the directory structure will look like this:"
    },
    {
      "type": "img",
      "src": "https://i.imgur.com/LsIuE3c.png",
      "alt": "Directory structure after successful download"
    },
    {
      "type": "p",
      "content": "Note that the size of this folder is **13 GB** !!!"
    },
    {
      "type": "p",
      "content": "The logs of a successful download will look like this:"
    },
    {
      "type": "img",
      "src": "https://i.imgur.com/YnhC8gG.png",
      "alt": "Logs of a successful download"
    },
    {
      "type": "p",
      "content": "This is what the logs of a successful download will look like!"
    },
    {
      "type": "h4",
      "className": "title",
      "content": "**Alternative Download Method and GPU Memory Usage**"
    },
    {
      "type": "p",
      "content": "Another method is to use the bittorrent link given in the readme of the pyllama repository."
    },
    {
      "type": "p",
      "content": "Size of the 7B model is 13GB (Unquantized). and running `nvidia-smi` told me that GPU memory usage is 13266MiB. Which is roughly 13.3 GB"
    },
    {
      "type": "h4",
      "className": "title",
      "content": "chatgpt response to \"My GPU memory usage is 13266MiB. How to interpret this\""
    },
    {
      "type": "p",
      "content": "The GPU memory usage of 13266MiB indicates that a portion of the memory on your graphics processing unit (GPU) is currently in use. Here's how you can interpret this information:"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "GPU Memory: GPUs have their own dedicated memory (VRAM) that is used to store data for processing graphics-related tasks. This memory is separate from the system memory (RAM) used by your computer's CPU. The GPU memory usage refers to the amount of VRAM being utilized at a given time."
        },
        {
          "type": "li",
          "content": "Unit of Measurement: \"MiB\" stands for mebibyte, which is a binary multiple of a megabyte. It is often used in reference to memory sizes. One mebibyte is equal to 1,048,576 bytes."
        },
        {
          "type": "li",
          "content": "Memory Usage: In your case, the GPU memory usage is 13266MiB, which means that approximately 13,266 mebibytes or roughly 13.3 gigabytes (GB) of the GPU's memory is currently in use."
        },
        {
          "type": "li",
          "content": "Implications: The GPU memory usage indicates the amount of memory that your GPU is actively utilizing to store data for ongoing graphics-related operations. It could include textures, shaders, buffers, or other resources needed for rendering images, running computations, or performing other GPU-accelerated tasks."
        },
        {
          "type": "li",
          "content": "Considerations: The amount of GPU memory usage can vary depending on the applications or processes running on your system. Graphics-intensive tasks such as gaming, video editing, 3D rendering, or machine learning can consume a significant amount of GPU memory. Monitoring GPU memory usage can help you understand how much memory your applications or workloads require and whether you are approaching the limits of your GPU's available memory."
        },
        {
          "type": "li",
          "content": "Limits and Optimization: GPUs have a maximum memory capacity, and if the memory usage exceeds this limit, it can lead to performance issues or even crashes. It's important to ensure that your GPU has enough available memory for the tasks you want to perform. If you encounter limitations due to memory constraints, you may need to optimize your application, reduce the resolution of textures, or consider upgrading to a GPU with higher memory capacity."
        }
      ]
    },
    {
      "type": "p",
      "content": "Overall, the GPU memory usage of 13266MiB indicates that a significant portion of your GPU's memory is currently being utilized by active processes or applications."
    },
    {
      "type": "h2",
      "className": "title",
      "content": "Code deep dive"
    },
    {
      "type": "p",
      "content": "In the official Code repository of LLama, the first thing that I noticed was that there were only 3 important code files. (This is obviously because they havent included the training code)"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "**llama/generation.py** : This file has a class which creates the pipeline for prompting (running inference) the model. This includes, sampling the top logits, custom stop function, pre and post processing of the input and output. [[code](https://github.com/facebookresearch/llama/blob/main/llama/generation.py)]"
        },
        {
          "type": "li",
          "content": "**llama/tokenizer.py** : Wraps the entencepeice tokenizer in a new class. [[code](https://github.com/facebookresearch/llama/blob/main/llama/tokenizer.py)]"
        },
        {
          "type": "li",
          "content": "**llama/model.py** : Holds the code for the transformer model [[code](https://github.com/facebookresearch/llama/blob/main/llama/model.py)]"
        }
      ]
    },
    {
      "type": "p",
      "content": "Lets start with the code for the Tokenizer"
    },
    {
      "type": "h2",
      "className": "title",
      "content": "Tokenizer"
    },
    {
      "type": "p",
      "content": "The job of the tokenizer is to assign an a numeric id to natural text. Which means after \"tokenizing\" this prompt - \"I believe the meaning of life is\" it will give us a tensor which looks like"
    },
    {
      "type": "p",
      "content": "`[[1, 306, 4658, 278, 6593, 310, 2834, 338]]` . The tokenizer is responsible for both encoding and decoding(coverting numeric id's) back to natural text. Also keep in mind that there is a tradoff between vocab size and the sequence length for a an input. What we want is a small enough vocab size which is representative of the entire corpus but also keeps the sequence length small. A large sequence lenght would mean more memory of the GPU consumed by the input!"
    },
    {
      "type": "p",
      "content": "As mentioned in the paper, they have used [sentencepeice's](https://github.com/google/sentencepiece) (Google's brainchild) implementation of the **Byte-Pair Encoding subword tokenization** algorithm, which means instead of encoding entire words they break it down into smaller syllabus. For example \"Tokenizer\" may be broken down into \"token\" and    \"-izer\" . In this way their vocabulary size is of 32,000 words. Similarly the entire coprus of text data consists of 1.4 Trillion tokens"
    },
    {
      "type": "blockquote",
      "content": "We tokenize the data with the byte- pair encoding (BPE) algorithm ([Sennrich et al., 2015](https://arxiv.org/abs/1508.07909)), using the implementation from Sentence- Piece ([Kudo and Richardson, 2018](https://arxiv.org/abs/1808.06226)). Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "from sentencepiece import SentencePieceProcessor\nfrom logging import getLogger\nfrom typing import List\nimport os\n\nlogger = getLogger()\nclass Tokenizer:\n    def __init__(self, model_path: str):\n        # reload tokenizer\n        assert os.path.isfile(model_path), model_path\n        self.sp_model = SentencePieceProcessor(model_file=model_path)\n        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n\n        # BOS / EOS token IDs\n        self.n_words: int = self.sp_model.vocab_size() # 32,000\n        self.bos_id: int = self.sp_model.bos_id()\n        self.eos_id: int = self.sp_model.eos_id()\n        self.pad_id: int = self.sp_model.pad_id()\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n\n    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n        assert type(s) is str\n        t = self.sp_model.encode(s)\n        if bos:\n            t = [self.bos_id] + t\n        if eos:\n            t = t + [self.eos_id]\n        return t\n\n    def decode(self, t: List[int]) -> str:\n        return self.sp_model.decode(t)"
        }
      ]
    },
    {
      "type": "p",
      "content": "The Tokenizer class isn't all that complicated. The parameter `model_path` holds the path to the `tokenizer.model` file which was downloaded along with the model weights."
    },
    {
      "type": "p",
      "content": "In the given code, BOS (Beginning of Sentence), EOS (End of Sentence), and Pad IDs (Padding IDs) have the following significance:"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "BOS ID: The BOS ID represents the token ID for the \"Beginning of Sentence\" token. It is used to indicate the start of a sentence or sequence. In the code, the **`encode`** function checks if the **`bos`** flag is True. If it is, the BOS ID is added at the beginning of the tokenized sequence."
        },
        {
          "type": "li",
          "content": "EOS ID: The EOS ID represents the token ID for the \"End of Sentence\" token. It is used to indicate the end of a sentence or sequence. In the code, the **`encode`** function checks if the **`eos`** flag is True. If it is, the EOS ID is added at the end of the tokenized sequence."
        },
        {
          "type": "li",
          "content": "Pad ID: The Pad ID represents the token ID for the \"Padding\" token. Padding is often used to make all sequences in a batch have the same length. In the code, the Pad ID is retrieved from the SentencePiece model using **`self.sp_model.pad_id()`**. It is typically used during batching and padding sequences to ensure uniform dimensions."
        }
      ]
    },
    {
      "type": "p",
      "content": "The BOS and EOS IDs help in marking the boundaries of sentences or sequences, which can be useful for various natural language processing tasks such as machine translation, text generation, and language modeling. The Pad ID ensures that sequences are of the same length when batching, which is necessary for efficient computation in deep learning models."
    },
    {
      "type": "blockquote",
      "content": "Here the values of bos_id, eos_id and pad_id is (1,2 and -1 correspondingly). Can be checked after creating an object of the Tokenizer class."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Byte pair encoding algorithm (Not Necessary to understand LLaMA)"
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "Personally, this a really smart,simple algorithm!!"
        },
        {
          "type": "li",
          "content": "First the entire corpus is divided into indiviudal characters and a counter is attached to each word, which indicates how many times the word has appeared in the corpus. Each character is now already a part of the Final Vocabulary"
        },
        {
          "type": "li",
          "content": "Then each word is divided into its characters and the pairwise occurence of each consecutive characters is counted. The most frequently occuring pair is added to the final corpus. In the sea of characters, wherever these 2 characters were occuring is combined into one and the process of counting the occurences of each pair is repeated and the most frequent one is added to the final Vocab."
        },
        {
          "type": "li",
          "content": "Honestly, just watch [this](https://www.youtube.com/watch?v=HEikzVL-lZU) video by hugging face if you didnt understand my explaination ðŸ˜Ÿ"
        }
      ]
    },
    {
      "type": "h2",
      "className": "title",
      "content": "Model"
    },
    {
      "type": "p",
      "content": "Before we dive deeper into the architecure and the working of the LLaMA transformer model,It is important to know what the model args for the 7B parameter look like:"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "ModelArgs(\n\tdim=4096, # An internal dimension for the transformer architecture\n\tn_layers=32, # Number of transformer blocks\n\tn_heads=32, # Number of heads\n\tvocab_size=32000, # Vocab size of the tokenizer\n\tmultiple_of=256, # A param used in calculating the positional embeddings\n\tnorm_eps=1e-06, # eps value used when dividing by zero during Normalisation\n\tmax_batch_size=1, # Max batch size during inferencing\n\tmax_seq_len=1024, # input to the transformer model can be this long only\n)"
        }
      ]
    },
    {
      "type": "p",
      "content": "In the code these params are stored in the `params.json` file, that was downloaded along with the checkpoint path"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "import json\nmax_seq_len = 1024\nmax_batch_size = 1\n\nwith open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n        params = json.loads(f.read())\n\n# create a model args object\n# ModelArgs is a a simple dataclass that contains the parameters for the model\nmodel_args: ModelArgs = ModelArgs(\n    max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n)\nmodel_args.vocab_size = tokenizer.n_words\n\n# model is loaded through the checkpoint by \n# ckpt_dir is the path to the `7B` directory in the folder downloaded from the llama.download filea\ncheckpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\nckpt_path = checkpoints[0]\ncheckpoint = torch.load(ckpt_path, map_location=\"cpu\")\n\nmodel = Transformer(model_args)\nmodel.load_state_dict(checkpoint, strict=False)"
        }
      ]
    },
    {
      "type": "p",
      "content": "After the model is loaded you can count the parameters by"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "# count number of parameters in model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\ncount_parameters(model)\n# Output\n# 6_738_415_616 (6.7 Billion)"
        }
      ]
    },
    {
      "type": "p",
      "content": "Now, We can see that in the [llama/model.py](https://github.com/facebookresearch/llama/blob/main/llama/model.py) file, there are 5 Classes â†’"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "[RMSNorm()](https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#LL33C7-L33C14) : Defines the Normalisation technique used in the model"
        },
        {
          "type": "li",
          "content": "[Attention()](https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L76) : Has the code for the MultiHead Self Attention Module"
        },
        {
          "type": "li",
          "content": "[FeedForward ()](https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L153) : Has the code for the MLP attached after each attention layer"
        },
        {
          "type": "li",
          "content": "[TransformerBlock()](https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L178) : Defines one transformer Block, there are total 32 of these in the architecture"
        },
        {
          "type": "li",
          "content": "[Transformer()](https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L198) : Has the embedding layer and code for instantiating the TransformerBlocks"
        }
      ]
    },
    {
      "type": "p",
      "content": "We'll go over each of these in detail"
    },
    {
      "type": "h4",
      "className": "title",
      "content": "Transformer()"
    },
    {
      "type": "p",
      "content": "There are several variants of transformer language models. LLaMA is an **autoregressive, decoder-only transformer** language models, such as GPT-3. Auto regressive because this model predicts future values of a variable based on its own past values (A term taken from time series analysis). Decoder and Encoder are two parts of the transformer architecture. Encoder is used for building a good understanding of the data, whereas decoder is used for more supervised task like, next token prediction, which is what the LLaMA model does. It is one big model that predicts the next token based on the prompt (context) that we give it."
    },
    {
      "type": "p",
      "content": "This encoder starts with an Embedding Layer followed by Multiple residual blocks of Multi Head Self Attention and Feed Forward layers. Each residual block consists of an attention layer, followed by an MLP layer. Both the attention and MLP layers each \"read\" their input from the residual stream (by performing a linear projection), and then \"write\" their result to the residual stream by adding a linear projection back in. Each attention layer consists of multiple heads, which operate in parallel. RMS Normalisation technique is used before any attention layer or feed forward layer."
    },
    {
      "type": "h4",
      "className": "title",
      "content": "Complete Code for Transformer()"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "class Transformer(nn.Module):\n    def __init__(self, params: ModelArgs):\n        super().__init__()\n        self.params = params # ModelArgs\n        self.vocab_size = params.vocab_size # 32_000\n        self.n_layers = params.n_layers # 32\n\n        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim) # (32_000, 4096)\n\n        self.layers = torch.nn.ModuleList()\n        for layer_id in range(params.n_layers):\n            self.layers.append(TransformerBlock(layer_id, params))\n\n        self.norm = RMSNorm(params.dim, eps=params.norm_eps) # shape of output is same as input\n        self.output = nn.Linear(params.dim, params.vocab_size, bias=False) # (4096, 32_000)\n\n        self.freqs_cis = precompute_freqs_cis(\n            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n            # 4096 // 32 = 128, 1024 * 2\n        ) # torch.Size([2048, 64])\n\n    @torch.inference_mode()\n    def forward(self, tokens: torch.Tensor, start_pos: int):\n        _bsz, seqlen = tokens.shape # (1,8)\n        h = self.tok_embeddings(tokens) # (1,8,4096)\n        self.freqs_cis = self.freqs_cis.to(h.device)\n        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen] # torch.Size([1024, 64])\n\n        mask = None\n        if seqlen > 1:\n            mask = torch.full(\n                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n            ) # (1,1,8,8) , filled with -inf\n            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n            # (1,1,8,8) , filled with -inf, but only the upper triangle, lower triangle is 0\n            # diagnol = start_pos + 1, so the first 8 tokens are not masked, it basically pushes the diagonola above\n\n        for layer in self.layers:\n            h = layer(h, start_pos, freqs_cis, mask)\n        h = self.norm(h) # (1,8,4096)\n        output = self.output(h[:, -1, :])  # only compute last logits # (1, 4096) * (4096, 32_000) = (1, 32_000)\n        return output.float() # (1, 32_000)"
        }
      ]
    },
    {
      "type": "p",
      "content": "The input to the transformer forward() function will always be a 2D tensor (batch-size,seq_len), in our case (1,8) . The batch size will always be fixed (1 for inferencing). For simplicity, lets take the batch size as 1 from here on. Seq-length will always keep on varying depending on the prompt, but after passing the prompt throught the transformer once, we only need to give it the next token predicted until the EOS token is predicted. So after the first iteration sequence length will always be 1. For now lets take the `seq_length` to be 1"
    },
    {
      "type": "p",
      "content": "The Emebdding layer ( self.tok_embeddings )is simply responsible for selecting the embeddings based on the indices passed as input to it. Therefore its size will always be (batch_size,seq_length,params.dim).  In our case, the output of the embedding layer becomes `(1,8,4096)` Note that the internal dimension used for 7B model is 4096."
    },
    {
      "type": "p",
      "content": "Lets look at the positional Embeddings used in this LLaMA architecture now."
    },
    {
      "type": "h4",
      "className": "title",
      "content": "Rotary Positional Embeddings"
    },
    {
      "type": "p",
      "content": "`self.freq_cis` is a tensor (fixed and not a trainable parameter) given by the `precompute_freq_cis` function. Instead of using absolute or trainable positional encodings, LLaMA uses Rotary positional encodings, which are know to have faster convergence and better results on various sequence modelling tasks."
    },
    {
      "type": "p",
      "content": "Rotary encoding transforms pairs of features by rotating in the 2D plane. That is, it organizes the *d* features as 2*d* pairs. Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it by an angle depending on the position of the token."
    },
    {
      "type": "p",
      "content": "For now its enough to understand that the function returns a 2D tensor of shape (2048,64) or (2*max_seq_length , params.dim // n_heads), where each element is of the form"
    },
    {
      "type": "img",
      "src": "https://i.imgur.com/tPHF68t.png",
      "alt": "iota3"
    },
    {
      "type": "p",
      "content": "Here `theta` is given by"
    },
    {
      "type": "img",
      "src": "https://i.imgur.com/6RifwFd.png",
      "alt": "iota2"
    },
    {
      "type": "h4",
      "className": "title",
      "content": "Code for generating positional embeddings"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device)  # type: ignore\n    freqs = torch.outer(t, freqs).float()  # type: ignore\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis"
        }
      ]
    },
    {
      "type": "h4",
      "className": "title",
      "content": "Here is a chatGPT generated response to what the `precompute_freq_cis()` does step by step"
    },
    {
      "type": "p",
      "content": "The function **`precompute_freqs_cis`** is used to precompute frequency sinusoids (**`freqs_cis`**) that are later used in the transformer architecture. These sinusoids play a role in applying rotational embeddings to the query and key representations, which is a technique used to introduce positional information to the transformer model. Let's break down the function step by step:"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "**`freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))`**:\n- **`torch.arange(0, dim, 2)[: (dim // 2)]`** generates a tensor of values **`[0, 2, 4, ..., dim-2]`**. It selects only the first **`dim // 2`** elements.\n- **`torch.arange(0, dim, 2)[: (dim // 2)].float() / dim`** divides the selected tensor by **`dim`** to obtain a range of values between 0 and 1.\n- **`(theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))`** raises **`theta`** to the power of each element in the range of values, resulting in a tensor of frequency factors.\n- **`1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))`** computes the reciprocal of each element in the tensor, resulting in a tensor of frequencies."
        },
        {
          "type": "li",
          "content": "**`t = torch.arange(end, device=freqs.device)`**:\n- **`torch.arange(end)`** generates a tensor with values ranging from 0 to **`end-1`**.\n- **`device=freqs.device`** ensures that the generated tensor is placed on the same device as the **`freqs`** tensor."
        },
        {
          "type": "li",
          "content": "**`freqs = torch.outer(t, freqs).float()`**:\n- **`torch.outer(t, freqs)`** computes the outer product between **`t`** and **`freqs`**, resulting in a 2D tensor where each element is the product of corresponding elements from **`t`** and **`freqs`**.\n- **`freqs = torch.outer(t, freqs).float()`** converts the resulting tensor to the **`float`** data type."
        },
        {
          "type": "li",
          "content": "**`freqs_cis = torch.polar(torch.ones_like(freqs), freqs)`**:\n- **`torch.ones_like(freqs)`** creates a tensor with the same shape as **`freqs`** and fills it with ones.\n- **`torch.polar(torch.ones_like(freqs), freqs)`** converts the ones tensor and **`freqs`** tensor to polar coordinates, resulting in a complex tensor **`freqs_cis`** where the magnitude is 1 and the phase is determined by the corresponding values in **`freqs`**."
        },
        {
          "type": "li",
          "content": "**`return freqs_cis`**: The function returns the computed frequency sinusoids **`freqs_cis`** as the final output."
        }
      ]
    },
    {
      "type": "p",
      "content": "Overall, the **`precompute_freqs_cis`** function is used to generate frequency sinusoids that are used in the transformer architecture to introduce positional information through rotational embeddings. These embeddings play a crucial role in capturing the relative positions of tokens within the input sequences, allowing the transformer to handle sequential data effectively."
    },
    {
      "type": "p",
      "content": "For better understanding refer to this annotated blog on [Rotaray Positional Embeddings](https://nn.labml.ai/transformers/rope/index.html) and the [original paper](https://arxiv.org/abs/2104.09864)."
    },
    {
      "type": "p",
      "content": "As to how these positional embeddings are used in the attention architecture, we'll come back to it later."
    },
    {
      "type": "p",
      "content": "The embeddings for the sequence, along with it's mask and starting position are fed to the 32 consecutive `TransformerBlock()` layers.  One thing to note here is that the input and output to the Attention layer  and the feed forward layer are always the same shape. Attention mechanism can be though of as how much more expressive you can make your sequence after each operation. Also Attention and feed forward layer are residual layers, which means that the the input to the attention layer is always added back to the output before they are collectively sent to the next attention layer."
    },
    {
      "type": "p",
      "content": "This is call Residual or Skip Connections and are done so as to allow the gradients to flow back more easily.  [[youtube video of Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5208s) explaining this concept]"
    },
    {
      "type": "p",
      "content": "After passing the input through all the `TransformerBlocks()` layers, one final normalisation (RMSNorm) is done before using a Linear layer to predicting the next token for the given input sequence. The Linear layer is of shape `(4096,32_000)`."
    },
    {
      "type": "p",
      "content": "Only the last logits  `h[:, -1, :]` to the input sequence is used as. input the the linear layer. With the laws of matrix multiplication we can see that the output of the entire Transformer architecture will always be a tensor of shape `(1,32_000)` . `(1, 4096) * (4096, 32_000) = (1, 32_000)`. These logits represent the model's confidence scores for each token being the next token in the sequence."
    },
    {
      "type": "p",
      "content": "The logits produced by the output layer are typically passed through a softmax function to convert them into probabilities. The softmax function normalizes the logits and assigns a probability distribution over the vocabulary. The next token prediction is made by selecting the token with the highest probability as the predicted next token."
    },
    {
      "type": "p",
      "content": "The decision to use only the last logits as input to the output layer is a common practice in many sequence-to-sequence tasks. The intuition behind this is that the last hidden state of the sequence, after undergoing multiple layers of attention and transformation, is expected to contain the most relevant and comprehensive information for generating the final output."
    },
    {
      "type": "p",
      "content": "By selecting the last hidden state, the model effectively focuses on the most recent context and conditions the output generation on the entire input sequence while taking advantage of the context modeling capabilities of the Transformer decoder."
    },
    {
      "type": "p",
      "content": "However, it's important to note that without the full context of the previous hidden states, the model may lose some information that could potentially be useful for certain tasks. Different model architectures or downstream tasks may require different strategies for leveraging the entire sequence of hidden states, and the choice of using only the last logits depends on the specific requirements of the task at hand."
    },
    {
      "type": "p",
      "content": "Now lets dive deeper into the class `TransformerBlock()`"
    },
    {
      "type": "h3",
      "className": "title",
      "content": "TransformerBlock()"
    },
    {
      "type": "p",
      "content": "Complete Code"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "class TransformerBlock(nn.Module):\n    def __init__(self, layer_id: int, args: ModelArgs):\n        super().__init__()\n        self.n_heads = args.n_heads\n        self.dim = args.dim\n        self.head_dim = args.dim // args.n_heads\n        self.attention = Attention(args)\n        self.feed_forward = FeedForward(\n            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n            # 4096, 4 * 4096, 256\n        )\n        self.layer_id = layer_id\n        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n\n    def forward(\n        self,\n        x: torch.Tensor, # (1,8,4096)\n        start_pos: int, # 0 (initially)\n        freqs_cis: torch.Tensor, # (8, 64)\n        mask: Optional[torch.Tensor], # (1,1,8,8)\n    ):\n        # this is a skip connection\n        h = x + self.attention.forward(\n            self.attention_norm(x), start_pos, freqs_cis, mask\n            # (1,8,4096), 0, (1024, 64), (1,1,8,8)\n        ) # (1,8,4096)\n        out = h + self.feed_forward.forward(self.ffn_norm(h))\n        return out # (1,8,4096)"
        }
      ]
    },
    {
      "type": "p",
      "content": "This class just encapsulates the attention layer and feed forward layer and the code for residual connection can also be seen here `(h = x + attention(x))`  and `out = h + feed_forward(h)`"
    },
    {
      "type": "p",
      "content": "A few things to note here"
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "Normalisation (**RMSNorm**) is done to the input before it enters either the attention or the feed forward layer. In the original transformer paper, it was done after the layer."
        }
      ]
    },
    {
      "type": "h3",
      "className": "title",
      "content": "RMSNorm()"
    },
    {
      "type": "p",
      "content": "Complete Code"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "class RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight"
        }
      ]
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "This Class performs the RMS normalization on the input tensor **`x`**."
        },
        {
          "type": "li",
          "content": "It computes the square of **`x`** using **`x.pow(2)`**, calculates the mean along the last dimension using **`mean(-1, keepdim=True)`**, and adds **`self.eps`** to the mean value for numerical stability."
        },
        {
          "type": "li",
          "content": "Then, it applies reciprocal square root (**`rsqrt`**) to the sum of the mean and **`self.eps`**."
        },
        {
          "type": "li",
          "content": "Finally, it multiplies **`x`** with the reciprocal square root to normalize the input."
        },
        {
          "type": "li",
          "content": "In short it is dividing the input by its L2 Norm of the last dimension (4096)"
        },
        {
          "type": "li",
          "content": "The normalization is computed using a learnable parameter **`self.weight`** and the **`eps`** value to ensure numerical stability. The module helps to normalize the input data and can be useful in certain neural network architectures to improve model performance and convergence."
        }
      ]
    },
    {
      "type": "h3",
      "className": "title",
      "content": "FeedForward()"
    },
    {
      "type": "p",
      "content": "Complete Code"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "class FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim: int, # 4096\n        hidden_dim: int, # 4 * 4096 = 16384\n        multiple_of: int, # 256\n    ):\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3) # 2 * 16384 / 3 = 10922\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) # 256 * (10922 + 256 - 1) // 256 = 11177\n\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False) # (4096, 11177)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False) # (11177, 4096)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False) # (4096, 11177)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x)) # (1,8,4096)"
        }
      ]
    },
    {
      "type": "p",
      "content": "In summary, the **`FeedForward`** module applies a series of linear transformations to the input tensor **`x`** using three linear layers (**`self.w1`**, **`self.w2`**, and **`self.w3`**) with appropriate dimensions. The intermediate activation function **`F.silu`** ([Sigmoid Linear unit](https://pytorch.org/docs/stable/generated/torch.nn.functional.silu.html)) is applied element-wise to introduce non-linearity."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Attention()"
    },
    {
      "type": "p",
      "content": "Complete Code"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "class Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        self.n_local_heads = args.n_heads // 1 # 32 // 1 = 32\n        self.head_dim = args.dim // args.n_heads # 4096 // 32 = 128\n\n        self.wq = nn.Linear(\n            args.dim,\n            args.n_heads * self.head_dim,\n            bias=False,\n        ) # (4096, 4096)\n        self.wk = nn.Linear(\n            args.dim,\n            args.n_heads * self.head_dim,\n            bias=False,\n        ) # (4096, 4096)\n        self.wv = nn.Linear(\n            args.dim,\n            args.n_heads * self.head_dim,\n            bias=False,\n        )  # (4096, 4096)\n        self.wo = nn.Linear(\n            args.n_heads * self.head_dim,\n            args.dim,\n            bias=False,\n        ) # (4096, 4096)\n        self.cache_k = torch.zeros(\n            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n            # (1,1024,32,128)\n        )\n        self.cache_v = torch.zeros(\n            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n            # (1,1024,32,128)\n        )\n        if hiq.get_env_bool(\"KV_CAHCHE_IN_GPU\", True):\n            self.cache_k = self.cache_k.cuda()\n            self.cache_v = self.cache_v.cuda()\n\n    def forward(\n        self,\n        x: torch.Tensor, # (1,8,4096)\n        start_pos: int, # 0 (initially)\n        freqs_cis: torch.Tensor,  # (8, 64)\n        mask: Optional[torch.Tensor],  # (1,1,8,8)\n    ):\n        bsz, seqlen, _ = x.shape\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n        # all of shape (1,8,4096)\n\n        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128) # (1,1,32,128)\n        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128) # (1,1,32,128)\n        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128) # (1,1,32,128)\n\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # (1,8,32,128), (1,8,32,128)\n        # # (1,1,32,128), (1,1,32,128)\n\n        self.cache_k = self.cache_k.to(xq) # (1,1024,32,128) moved to the same device as xq\n        self.cache_v = self.cache_v.to(xq) # (1,1024,32,128) moved to the smae device as xq\n\n        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk # (1,1024,32,128)\n        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv # (1,1024,32,128)\n\n        keys = self.cache_k[:bsz, : start_pos + seqlen] # (1,start_pos + seqlen,32,128) or (1,8,32,128)\n        values = self.cache_v[:bsz, : start_pos + seqlen] # (1,start_pos + seqlen,32,128) or (1,8,32,128)\n        # in the next run it will be (1,9,32,128) \n\n        xq = xq.transpose(1, 2) # (1,32,8,128) -> (1,32,1,128)\n        keys = keys.transpose(1, 2) # (1,32,8,128) -> (1,32,9,128)\n        values = values.transpose(1, 2) # (1,32,8,128) -> (1,32,9,128)\n        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) # (1,32,8,8)\n        # matrix multiply of (1,32,8,128) and (1,32,128,8) resulting in # (1,32,8,8)\n        # matrix multiply of (1,32,1,128) and (1,32,128,9) resulting in # (1,32,1,9)\n        if mask is not None: # (1,1,8,8) -> (1,1,1,1)\n            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen) # (1,32,8,8) -> (1,32,1,9)\n        scores = F.softmax(scores.float(), dim=-1).type_as(xq) # (1,32,8,8) -> (1,32,1,9)\n        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim) # (1,32,8,128)\n        # matrix multiply of (1,32,8,8) and (1,32,8,128) resulting in (1,32,8,128)\n        # matrix multiply of (1,32,1,9) and (1,32,9,128) resulting in (1,32,1,128)\n        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) # (1,8,4096) -> (1,1,4096)\n\n        return self.wo(output) # (1,8,4096) x (4096,4096) -> (1,8,4096)\n        # (1,1,4096) x (4096,4096) -> (1,1,4096)"
        }
      ]
    },
    {
      "type": "p",
      "content": "In summary, the **`Attention`** module implements the multi-head self-attention mechanism, which is a key component of transformer architectures. It processes the input tensor **`x`** through query, key, and value projections, applies rotary positional embeddings, and computes attention scores. The module also implements caching of key and value tensors to optimize performance in sequential processing scenarios."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "The Multi Head Self Attention mechanism"
    },
    {
      "type": "p",
      "content": "The Self Attention operator generates a better representation of the input sequence, where each token/embedding at the nth position in the sequence condenses the knowledge of the sequence coming before it. This can be thought of as a weighted average of all the tokens coming before it."
    },
    {
      "type": "p",
      "content": "In the self attention mechanism, we gather information from the past in a data-dependent way. We first generate a query, key, and value from each token by multiplying each token by 3 separate weight matrices. This converts one token into 3 representations using a linear layer."
    },
    {
      "type": "p",
      "content": "The process then involves taking the scaled dot product of one query vector with each key vector, multiplying this scalar with its value vector, and adding all these value vectors to get the representation of the token for which the query vector was generated."
    },
    {
      "type": "p",
      "content": "This mechanism allows the new representation to include all of the value vectors weighted according to their relevant information. It effectively chooses what to include and how much knowledge to incorporate. The query vector can be thought of as \"what I'm looking for\", the key vector as \"what I contain\", and the value vector as \"if you find me interesting, here's what I will communicate to you\"."
    },
    {
      "type": "p",
      "content": "Multihead attention creates `n_head` different q,k,v vectors and lets them operate in parallel, similar to using multiple filters in CNNs. This allows different information and contexts to be learned in each head. The outputs are then concatenated and multiplied with another weight matrix, which is fed as input to the next attention layer."
    },
    {
      "type": "blockquote",
      "content": "It's not always necessary to prevent the current token from looking at future tokens, such as in encoders or sentiment analysis. That's why masking is used here to ensure that the value vectors for the tokens ahead in the sequence are not added to its value representation."
    },
    {
      "type": "p",
      "content": "Other excellent resources to understand the Attention mechanism are:"
    },
    {
      "type": "ul",
      "content": [
        {
          "type": "li",
          "content": "[Karpathy's NanoGPT tutorial](https://youtu.be/kCc8FmEb1nY)"
        },
        {
          "type": "li",
          "content": "[The annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
        },
        {
          "type": "li",
          "content": "[In depth transformers](https://transformer-circuits.pub/)"
        },
        {
          "type": "li",
          "content": "[Jay Allamar's Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)"
        }
      ]
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Caching in the attention mechanism"
    },
    {
      "type": "p",
      "content": "This concept was introduced in [this paper](https://arxiv.org/abs/2112.05682), which claims that the attention mechanism does not have to be an O(n^2) operation. Instead, by storing the key and value vectors in a cache generated in this sequence, we don't have to compute them again when we pass in the next input token."
    },
    {
      "type": "p",
      "content": "The paper states:"
    },
    {
      "type": "blockquote",
      "content": "First, we use an efficient implementation of the causal multi-head attention to reduce memory usage and runtime. This implementation, available in the [xformers](https://github.com/facebookresearch/xformers) library, is inspired by [Rabe and Staats (2021)](https://arxiv.org/abs/2112.05682) and uses the backward from Dao et al. (2022). This is achieved by not storing the attention weights and not computing the key/query scores that are masked due to the causal nature of the language modeling task."
    },
    {
      "type": "p",
      "content": "As you can see here, we simply query the cache to get the keys and values from the past:"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "keys = self.cache_k[:bsz, : start_pos + seqlen] # (1,start_pos + seqlen,32,128) or (1,8,32,128)\nvalues = self.cache_v[:bsz, : start_pos + seqlen] # (1,start_pos + seqlen,32,128) or (1,8,32,128)\n# in the next run it will be (1,9,32,128)"
        }
      ]
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Masking in attention mechanism"
    },
    {
      "type": "p",
      "content": "During each forward run, if the seq_length passed in is more than 1, a mask is generated which is a tensor of shape (1,1,seq_length,seq_length). In this tensor, the upper triangle is filled with `-inf` and the lower triangle is filled with zeros. When the weight matrix is calculated (by multiplying keys and queries), this mask is added to the weight matrix. This makes the upper triangle of the weight matrix zero. When we take the softmax of this weight matrix, the `-inf` will convert to zeros and the rest of the values will be converted to probability values."
    },
    {
      "type": "p",
      "content": "This is done because we do not want the token at the nth position to contain information from the tokens ahead of itself. For predicting the next token, the embeddings after the starting_pos should not be \"visible\" to the previous_tokens. This mathematical trick is explained clearly in [Karpathy's tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2533s) on GPT."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Use of Rotary positional embeddings"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "def apply_rotary_emb(\n    xq: torch.Tensor, # (1,8,32,128)\n    xk: torch.Tensor, # (1,8,32,128)\n    freqs_cis: torch.Tensor, # (8,64)\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)) # (1,8,32,128) -> (1,8,32,64,2) -> (1,8,32,64)\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)) # (1,8,32,128) -> (1,8,32,64,2) -> (1,8,32,64)\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_) # (1,8,1,64)\n    # Element-wise multiplication of 2 matrices which has complex numbers\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3) # (1,8,32,64,2) -> (1,8,32,128) \n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3) # (1,8,32,64,2) -> (1,8,32,128)\n    return xq_out.type_as(xq), xk_out.type_as(xk) # (1,8,32,128), (1,8,32,128)"
        }
      ]
    },
    {
      "type": "p",
      "content": "The positional embeddings are tensors of complex numbers of the form cos(theta) + i sin(theta). Here, we first convert the query and key vectors into their complex forms by reshaping them, and then element-wise multiplication with the positional embeddings is done. These complex-numbered tensors are then converted back to their real form and returned."
    },
    {
      "type": "h2",
      "className": "title",
      "content": "Generation"
    },
    {
      "type": "p",
      "content": "At the core of the generate.py is this logic:"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "bsz = len(prompts) # 1\nparams = self.model.params # those same ModelArgs\nassert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n\nprompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n# [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n\nmin_prompt_size = min([len(t) for t in prompt_tokens]) # 8\nmax_prompt_size = max([len(t) for t in prompt_tokens]) # 8\n\ntotal_len = min(params.max_seq_len, max_gen_len + max_prompt_size) # 264\n\ntokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n# a tensor of size (1, 264) filled with -1's\n\nfor k, t in enumerate(prompt_tokens):\n    tokens[k, : len(t)] = torch.tensor(t).long()\n    # fill the first 8(length of prompt_tokens[0]) tokens of tokens with the prompt tokens\ninput_text_mask = tokens != self.tokenizer.pad_id # a tensor of size (1, 264) filled with True's ,\n# where tokens is not -1, other wise False\nstart_pos = min_prompt_size # 8\nprev_pos = 0\nfor cur_pos in range(start_pos, total_len):\n    i = tokens[:, prev_pos:cur_pos]\n    logits = self.model(i, prev_pos)\n    if temperature > 0:\n        probs = torch.softmax(logits / temperature, dim=-1)\n        next_token = sample_top_p(probs, top_p)\n    else:\n        next_token = torch.argmax(logits, dim=-1)\n    next_token = next_token.reshape(-1)\n    # only replace token if prompt has already been generated\n    next_token = torch.where(\n        input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n    )\n    tokens[:, cur_pos] = next_token\n    prev_pos = cur_pos\n    \n    if self._should_stop(tokens, prompt_tokens, stop_ids, stop_words):\n        break"
        }
      ]
    },
    {
      "type": "p",
      "content": "The `generate` function takes `prompts` as a list of strings. For example, let the prompt be \"**I believe the meaning of life is**\". `prompt_tokens` are generated through the tokenizer's encode function, and the minimum and maximum prompt sizes in the entire list of strings are calculated. In this case, both will be `8`."
    },
    {
      "type": "p",
      "content": "The maximum sequence length that can be passed to the transformer is `1024` (a fixed, predecided hyperparameter, also called the context length). We also don't want the output to be more than 256 tokens (again, a predetermined fixed hyperparameter)."
    },
    {
      "type": "p",
      "content": "Considering these two factors, the total length that will be generated is calculated by taking the minimum of `1024` and `256 + prompt_length`. In this case, it will be `264`."
    },
    {
      "type": "p",
      "content": "A tensor is created where the initial few values are our original `prompt_tokens` and the rest are filled with `pad_id (-1)`. Correspondingly, an `input_mask` of size (1, 264) is created, filled with True's where tokens are not -1, and False otherwise. In the future, only where the mask is -1 will be replaced with generated tokens."
    },
    {
      "type": "p",
      "content": "A loop is then run which iterates over the tokens. In the first pass, the entire prompt is fed into the model, with starting_pos as 0. From the next iteration onwards, only one token will be sent for forward pass to the model. This can be validated by checking the difference between the `prev_pos` and `cur_pos`. After the first iteration, the difference will always be 1."
    },
    {
      "type": "blockquote",
      "content": "Note that starting position is passed in to the transformer because that is used later in the caching in the self attention mechanism. And, after the first iteration as the sequence length will always be 1, there is no need for an attention mask."
    },
    {
      "type": "p",
      "content": "Regardless of the input size, the output will always be the logits of shape (1,32_000). Softmax is applied to convert them into probabilities, and finally, one token is sampled from the logits (considering it a multinomial distribution). As a new token is generated, it is added to the `tokens` variable depending on the input mask, until the tensor of shape 264 is completely filled. The loop might break in between if the model generates a token or a word that should not have been generated. `_should_stop()` takes care of this."
    },
    {
      "type": "p",
      "content": "After this, post-processing is done on the decoded tokens so that the output is more presentable and readable to the user. Let's look at three very important concepts here - **Temperature**, **Sampling from the top probabilities**, and the **post-processing function**."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "The role of temperature"
    },
    {
      "type": "p",
      "content": "The temperature determines how greedy the generative model is."
    },
    {
      "type": "p",
      "content": "If the temperature is low, the probabilities to sample classes other than the one with the highest log probability will be small, and the model will probably output the most correct text, but rather boring, with small variation."
    },
    {
      "type": "p",
      "content": "If the temperature is high, the model can output, with rather high probability, other words than those with the highest probability. The generated text will be more diverse, but there is a higher possibility of grammar mistakes and generation of nonsense."
    },
    {
      "type": "img",
      "src": "https://i.imgur.com/HCS0swK.png",
      "alt": "Temperature effect on categorical distribution"
    },
    {
      "type": "p",
      "content": "The difference between the low-temperature case (left) and the high-temperature (right) case for the categorical distribution is illustrated in the picture above, where the heights of the bars correspond to probabilities."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Sampling from top p probabilites"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "def sample_top_p(probs, # a tensor of size (1, 32_000) with softmax already applied\n                 p # 0.95\n                 ):\n    \"\"\" \n    In summary, the sample_top_p function performs top-p sampling on a given probability\n    distribution. It sorts the probabilities, computes the cumulative sum, applies a \n    threshold to remove tokens with cumulative probabilities exceeding the threshold, \n    normalizes the probabilities, samples a token using multinomial sampling, and returns \n    the sampled token index.\n\n    If the sum of probalities coming before a token is greater than p, then the token is\n    not considered for sampling. This is done by setting the probability of the token to 0.\n    \"\"\"\n    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    mask = probs_sum - probs_sort > p\n    probs_sort[mask] = 0.0\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    next_token = torch.multinomial(probs_sort, num_samples=1)\n    next_token = torch.gather(probs_idx, -1, next_token)\n    return next_token"
        }
      ]
    },
    {
      "type": "p",
      "content": "The `sample_top_p` function is used to perform top-p sampling on a probability distribution. It takes two inputs:"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "`probs`: A tensor of size `(1, 32_000)` representing a probability distribution over 32,000 tokens. The softmax function has already been applied to convert the logits into probabilities."
        },
        {
          "type": "li",
          "content": "`p`: A float representing the threshold probability. Tokens with cumulative probabilities above this threshold will be considered for sampling."
        }
      ]
    },
    {
      "type": "p",
      "content": "Here's a step-by-step explanation of the function:"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "`probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)`: The `probs` tensor is sorted in descending order along the last dimension (`dim=-1`). This sorts the probabilities from highest to lowest. `probs_sort` contains the sorted probabilities, and `probs_idx` contains the corresponding indices."
        },
        {
          "type": "li",
          "content": "`probs_sum = torch.cumsum(probs_sort, dim=-1)`: The cumulative sum of the sorted probabilities is computed along the last dimension using `torch.cumsum()`. This results in a tensor `probs_sum` where each element represents the cumulative sum of probabilities up to that point."
        },
        {
          "type": "li",
          "content": "`mask = probs_sum - probs_sort > p`: A boolean mask is created by comparing the difference between `probs_sum` and `probs_sort` with the threshold `p`. The mask will be `True` for tokens whose cumulative probability exceeds the threshold `p`."
        },
        {
          "type": "li",
          "content": "`probs_sort[mask] = 0.0`: The probabilities corresponding to the tokens that exceed the threshold are set to `0.0`. This effectively removes those tokens from consideration during sampling."
        },
        {
          "type": "li",
          "content": "`probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))`: The `probs_sort` tensor is divided by its sum along the last dimension (`dim=-1`). This normalizes the probabilities so that they sum up to 1, ensuring they form a valid probability distribution."
        },
        {
          "type": "li",
          "content": "`next_token = torch.multinomial(probs_sort, num_samples=1)`: Using `torch.multinomial()`, a single token is sampled from the probability distribution defined by `probs_sort`. The `num_samples` parameter is set to `1`, indicating that only one token is sampled."
        },
        {
          "type": "li",
          "content": "`next_token = torch.gather(probs_idx, -1, next_token)`: The sampled token index is obtained by using `torch.gather()` to gather the corresponding index from `probs_idx`. This ensures that the sampled token index matches the original indexing of the input tensor."
        },
        {
          "type": "li",
          "content": "`return next_token`: The sampled token index is returned as the output of the function."
        }
      ]
    },
    {
      "type": "p",
      "content": "In summary, the `sample_top_p` function performs top-p sampling on a given probability distribution. It sorts the probabilities, computes the cumulative sum, applies a threshold to remove tokens with cumulative probabilities exceeding the threshold, normalizes the probabilities, samples a token using multinomial sampling, and returns the sampled token index."
    },
    {
      "type": "h3",
      "className": "title",
      "content": "Post Processing function"
    },
    {
      "type": "pre",
      "content": [
        {
          "type": "code",
          "content": "def postprocessing(output_text, stop_words=None, threshold=10):\n    \n    sentences = output_text.split(\".\")\n    filtered_sentences = []\n    for sentence in sentences:\n        sentence = sentence.strip()\n        if len(sentence) > threshold and sentence[-1] == \".\":\n            filtered_sentences.append(sentence)\n    r = '.'.join(sentences).strip()\n    if stop_words:\n        for w in stop_words:\n            if r.endswith(w):\n                r = r[0:-len(w)].strip()\n    if r[-1] != '.':\n        r += '...'\n    return r"
        }
      ]
    },
    {
      "type": "p",
      "content": "The `postprocessing` function takes an `output_text` string and performs some post-processing steps to clean and format the text. Let's break down the function step by step:"
    },
    {
      "type": "ol",
      "content": [
        {
          "type": "li",
          "content": "Splitting into sentences: The `output_text` string is split into sentences using the period (\".\") as the delimiter. This is done with the `split()` method, which returns a list of individual sentences."
        },
        {
          "type": "li",
          "content": "Filtering sentences: Each sentence is processed individually in a loop. First, any leading or trailing whitespace is removed by using the `strip()` method. Then, the length of the sentence is checked against the `threshold` value. If the sentence is longer than the threshold and ends with a period (indicating a complete sentence), it is considered valid and added to the `filtered_sentences` list."
        },
        {
          "type": "li",
          "content": "Joining filtered sentences: The `filtered_sentences` list is joined back into a single string using the period as the separator. This step is performed with the `join()` method, resulting in a string that contains only the valid sentences."
        },
        {
          "type": "li",
          "content": "Handling stop words: If the `stop_words` parameter is provided, the function checks if the resulting string `r` ends with any of the stop words. If a stop word is found at the end of the string, it is removed by slicing the string from the beginning up to the length of the stop word. The resulting string is then stripped of any leading or trailing whitespace."
        },
        {
          "type": "li",
          "content": "Ensuring sentence ending: Finally, the function checks if the resulting string `r` ends with a period. If it doesn't, a period and ellipsis (\"...\") are appended to the string."
        }
      ]
    },
    {
      "type": "p",
      "content": "The purpose of this post-processing function is to clean up the generated text and ensure that it contains valid and properly formatted sentences. It also provides the flexibility to handle specific cases such as removing stop words and enforcing proper sentence endings."
    },
    {
      "type": "h2",
      "className": "title",
      "content": "Conclusion"
    },
    {
      "type": "p",
      "content": "Thats it!! I know this was a long blog. We looked at why the LLaMA models are relevant in today's world, we dived deep into the tokenizer it uses, what architectural changes it included in the classic transformer models and what is the mechanism behind generating a new token. If stuck somewhere considering having a look at my [fork](https://github.com/nishantb06/pyllama) of Pyllama package. The code here is heavily commented and I'm sure this will help. Please reach out on twitter [@itsnishant14](https://twitter.com/itsnishant14) incase of any questions or discrepancies."
    },
    {
      "type": "p",
      "content": "Thanks for giving this a read :)"
    }
  ]
}